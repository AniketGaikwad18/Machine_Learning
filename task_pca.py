# -*- coding: utf-8 -*-
"""Task-PCA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19AtHlobDsFzsOSXKl8K9qlsqNu2McXza

### `Task` How dimensionality reduction using Principal Component Analysis (PCA) on the Wine Quality dataset contributes to improving the classification accuracy and efficiency of wine type.

Note : Use KNN for Classification.

Data Link :  [Wine Data](https://docs.google.com/spreadsheets/d/e/2PACX-1vQDVwxneOKOaJL13QMhkAhYrgWlH1tICY7RacUnj_lL8m9uUWaaUf3p7bScNyh_D2Rvt7nc1q11adSy/pub?gid=647503637&single=true&output=csv)
"""

# Data Loading
import pandas as pd
wine_data_path = "https://docs.google.com/spreadsheets/d/e/2PACX-1vQDVwxneOKOaJL13QMhkAhYrgWlH1tICY7RacUnj_lL8m9uUWaaUf3p7bScNyh_D2Rvt7nc1q11adSy/pub?gid=647503637&single=true&output=csv"
wine = pd.read_csv(wine_data_path)
wine.head(1)

wine.sample(1)

wine.shape

import matplotlib.pyplot as plt
import seaborn as sns

wine.info()

wine.isna().sum()

wine.duplicated().sum()

# We need to remove duplicated rows and missing value rows
import pandas as pd

# Load the Wine Quality dataset
wine_data = pd.read_csv(wine_data_path)
print("Wine Data Shape :", wine_data.shape)
# Drop rows with missing values
wine_data = wine_data.dropna()

# Dropping Duplicates rows
wine_data.drop_duplicates(inplace=True)
print("Wine Data Shape (After Dropping-) :", wine_data.shape)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

x = wine_data.drop('type',axis=1)
y = wine_data['type']

X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

knn = KNeighborsClassifier()

knn.fit(X_train,y_train)

y_pred = knn.predict(X_test)
accuracy_score(y_test,y_pred)

"""With PCA"""

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.decomposition import PCA

"""#FOR 2D"""

pca = PCA(n_components=2)
X_train_trf = pca.fit_transform(X_train)
X_test_trf = pca.transform(X_test)

import plotly.express as px
y_train_trf = y_train.astype(str)
fig = px.scatter(x=X_train_trf[:,0],y=X_train_trf[:,1],color=y_train_trf)
fig.show()

"""#FOR 3D"""

pca = PCA(n_components=3)
X_train_trf = pca.fit_transform(X_train)
X_test_trf = pca.transform(X_test)

import plotly.express as px
y_train_trf = y_train.astype(str)
fig = px.scatter_3d(x=X_train_trf[:,0],y=X_train_trf[:,1]
,z = X_train_trf[:,2],color=y_train_trf)
fig.show()

"""CHECKING THE OPTIMAL SIZE"""

pca = PCA(n_components=None)
X_train_trf = pca.fit_transform(X_train)
X_test_trf= pca.transform(X_test)

X_train_trf.shape

"""HERE WE CAN SEE THE TOP 5 WILL GIVE THE 90% VARIANCE"""

pca.explained_variance_

import numpy as np
np.cumsum(pca.explained_variance_ratio_)

plt.plot(np.cumsum(pca.explained_variance_ratio_))

